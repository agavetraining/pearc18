{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting Stuff Up</h2>\n",
    "\n",
    "Here we import some packages that we'll need in various places. We'll also load all the variables we set in config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/agave\n",
      "Requirement already up-to-date: setvar in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "AGAVE_APP_NAME=training-dooley\n",
      "AGAVE_STORAGE_SYSTEM_ID=sandbox-storage-dooley\n",
      "AGAVE_EXECUTION_SYSTEM_ID=sandbox-exec-dooley\n",
      "AGAVE_SYSTEM_SITE_DOMAIN=localdomain\n",
      "MACHINE_NAME=sandbox\n",
      "MACHINE_USERNAME=jovyan\n",
      "AGAVE_STORAGE_HOME_DIR=/home/jovyan\n",
      "SCRATCH_DIR=/home/jovyan\n",
      "AGAVE_STORAGE_WORK_DIR=/home/jovyan\n",
      "AGAVE_APP_DEPLOYMENT_PATH=agave-deploy\n",
      "AGAVE_APP_DEPLOYMENT_PATH=agave-deploy\n",
      "AGAVE_APP_NAME=training-dooley\n",
      "AGAVE_EXECUTION_SYSTEM_ID=sandbox-exec-dooley\n",
      "AGAVE_STORAGE_HOME_DIR=/home/jovyan\n",
      "AGAVE_STORAGE_SYSTEM_ID=sandbox-storage-dooley\n",
      "AGAVE_STORAGE_WORK_DIR=/home/jovyan\n",
      "AGAVE_SYSTEM_SITE_DOMAIN=localdomain\n",
      "MACHINE_NAME=sandbox\n",
      "MACHINE_USERNAME=jovyan\n",
      "SCRATCH_DIR=/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/agave\n",
    "\n",
    "%cd ~/agave\n",
    "\n",
    "!pip3 install --upgrade setvar\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from agavepy.agave import Agave\n",
    "from setvar import *\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "# This cell enables inline plotting in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "setvar(\"\"\"\n",
    "AGAVE_APP_NAME=training-${AGAVE_USERNAME}\n",
    "AGAVE_STORAGE_SYSTEM_ID=sandbox-storage-${AGAVE_USERNAME}\n",
    "AGAVE_EXECUTION_SYSTEM_ID=sandbox-exec-${AGAVE_USERNAME}\n",
    "AGAVE_SYSTEM_SITE_DOMAIN=localdomain\n",
    "MACHINE_NAME=sandbox\n",
    "MACHINE_USERNAME=jovyan\n",
    "AGAVE_STORAGE_HOME_DIR=/home/${MACHINE_USERNAME}\n",
    "SCRATCH_DIR=/home/${MACHINE_USERNAME}\n",
    "AGAVE_STORAGE_WORK_DIR=/home/${MACHINE_USERNAME}\n",
    "AGAVE_APP_DEPLOYMENT_PATH=agave-deploy\n",
    "\"\"\")\n",
    "loadvar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenant configuration  \n",
    "\n",
    "If you are running against a privately hosted Agave tenant, you need to configure the tutorial to run against your tenant. You can do that by setting the `AGAVE_TENANTS_API_BASEURL` and `AGAVE_TENANT_ID` environment variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you specified the correct address to your Tenants API, you should be able to discover the tenants available for you through the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0msandbox\u001b[0m\n",
      "\u001b[1;0mToken for sandbox:dooley successfully refreshed and cached for 8223 seconds\n",
      "c82836b10136a1543b7939da720d94\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!tenants-list\n",
    "!auth-tokens-create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the tenant you would like to use by setting the `AGAVE_TENANT` environment variable. You may select either the name of the tenant, or leave it blank to select the default tenant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mYou are now configured to interact with the APIs at https://sandbox.agaveplatform.org/\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!tenants-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the Client</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step we delete the client if it exists. Chances are, yours doesn't yet. We put this command here in case, for some reason, you want to re-create your client later on. If you delete the client you intend to create before you create it, no harm is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully deleted client training-dooley\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!clients-delete -u \"$AGAVE_USERNAME\" -p \"$AGAVE_PASSWORD\" $AGAVE_APP_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we create the client. Clients provide a way of encapsulating resources connected to a single project. Through the client, you will receive a token which you can use to run most of the Agave commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully created client training-dooley\r\n",
      "key: KrTTfIZ1OQ4Hie3ecO6aFK88LhEa \r\n",
      "secret: EuaogZcXq3_jMvGQCOQqtCNtgiQa\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!clients-create -u \"$AGAVE_USERNAME\" -p \"$AGAVE_PASSWORD\" -N \"$AGAVE_APP_NAME\" -S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the token for your client. You will, from this point on, use this token to run the remainder of the Agave commands in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mToken for sandbox:dooley successfully refreshed and cached for 14400 seconds\r\n",
      "342f8cbacc5d8c60c5944bf9f35451b3\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!auth-tokens-create -u $AGAVE_USERNAME -p \"$AGAVE_PASSWORD\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOLLOWING ALONG AT HOME  \n",
    "\n",
    "If you are following along at home using the docker-compose stack, you will need to run the following cell to get the hostname and port of your tcp tunnel so Agave can contact your system without a public IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.tcp.ngrok.io:16798\n",
      "Reading file `ngrok_port.txt'\n",
      "Reading file `ngrok_host.txt'\n",
      "Reading file `ngrok_host.txt'\n",
      "Reading file `ngrok_port.txt'\n",
      "VM_PORT=16798\n",
      "VM_MACHINE=0.tcp.ngrok.io\n",
      "VM_IPADDRESS=52.15.183.149\n"
     ]
    }
   ],
   "source": [
    "if os.environ.get('USE_TUNNEL') == 'True': \n",
    "    # fetch the hostname and port of the reverse tunnel running in the sandbox \n",
    "    # so Agave can connect to our local sandbox\n",
    "    !echo $(ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null sandbox 'curl -s  http://localhost:4040/api/tunnels | jq -r '.tunnels[0].public_url'') > ngrok_url.txt  \n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||'\n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||' | sed -r 's#(.*):(.*)#\\1#' > ngrok_host.txt\n",
    "    !cat ngrok_url.txt | sed 's|^tcp://||' | sed -r 's#(.*):(.*)#\\2#' > ngrok_port.txt\n",
    "\n",
    "    # set the environment variables otherwise set when running in a training cluster\n",
    "    os.environ['VM_PORT'] = readfile('ngrok_port.txt').strip()\n",
    "    os.environ['VM_MACHINE'] = readfile('ngrok_host.txt').strip()\n",
    "    os.environ['AGAVE_SYSTEM_HOST'] = readfile('ngrok_host.txt').strip()\n",
    "    os.environ['AGAVE_SYSTEM_PORT'] = readfile('ngrok_port.txt').strip()\n",
    "    !echo \"VM_PORT=$VM_PORT\"\n",
    "    !echo \"VM_MACHINE=$VM_MACHINE\"\n",
    "    setvar(\"VM_IPADDRESS=$(getent hosts ${VM_MACHINE}|cut -d' ' -f1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Storage Machine   \n",
    "\n",
    "Agave wants to know which place (or places) you want to store the data associated with your jobs. Here, we're going to set that up. Authentication to the storage machine will be through SSH keys. The key and public key files, however, contain newlines. To encode them in Json (the data format used by Agave), we will run the jsonpki command on each file. Next, we will store its contents in the environment for use by setvar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/key\n",
    "!chmod 700 ~/key\n",
    "!jsonpki --public ~/.ssh/id_rsa.pub > ~/key/id_rsa.pub.txt\n",
    "!jsonpki --private ~/.ssh/id_rsa > ~/key/id_rsa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file `/home/jovyan/key/id_rsa.pub.txt'\n",
      "Reading file `/home/jovyan/key/id_rsa.txt'\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PUB_KEY\"]=readfile(\"${HOME}/key/id_rsa.pub.txt\").strip()\n",
    "os.environ[\"PRIV_KEY\"]=readfile(\"${HOME}/key/id_rsa.txt\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next cell, we create the json file used to describe the storage machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `sandbox-storage-dooley.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"${AGAVE_STORAGE_SYSTEM_ID}.txt\",\"\"\"{\n",
    "    \"id\": \"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "    \"name\": \"${MACHINE_NAME} storage (${MACHINE_USERNAME})\",\n",
    "    \"description\": \"The ${MACHINE_NAME} computer\",\n",
    "    \"site\": \"${AGAVE_SYSTEM_SITE_DOMAIN}\",\n",
    "    \"type\": \"STORAGE\",\n",
    "    \"storage\": {\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SFTP\",\n",
    "        \"rootDir\": \"/\",\n",
    "        \"homeDir\": \"${AGAVE_STORAGE_HOME_DIR}\",\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we tell Agave about the machine. You can re-run the previous cell and the next one if you want to change the definition of your storage machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added system sandbox-storage-dooley\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!systems-addupdate -F ${AGAVE_STORAGE_SYSTEM_ID}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the Agave command `files-list`. This provides a check that we've set up the storage machine correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n",
      "/home/jovyan/.bash_logout\r\n",
      "/home/jovyan/.bashrc\r\n",
      "/home/jovyan/.cache\r\n",
      "/home/jovyan/.ngrok2\r\n"
     ]
    }
   ],
   "source": [
    "!files-list -l 5 -S ${AGAVE_STORAGE_SYSTEM_ID} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting up the Execution Machine</h2>\n",
    "\n",
    "You may not always wish to store your data on the same machine you run your jobs on. However, in this tutorial, we will assume that you do. The description for the execution machine is much like the storage machine. However, there are a few more pieces of information you'll need to provide. In this example, we are going to call commands directly on the host as opposed to using a batch queue scheduler. It is slightly simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `sandbox-exec-dooley.txt'\n"
     ]
    }
   ],
   "source": [
    "# Edit any parts of this file that you know need to be changed for your machine.\n",
    "writefile(\"${AGAVE_EXECUTION_SYSTEM_ID}.txt\",\"\"\"\n",
    "{\n",
    "    \"id\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "    \"name\": \"${MACHINE_NAME} (${MACHINE_USERNAME})\",\n",
    "    \"description\": \"The ${MACHINE_NAME} computer\",\n",
    "    \"site\": \"${AGAVE_SYSTEM_SITE_DOMAIN}\",\n",
    "    \"public\": false,\n",
    "    \"status\": \"UP\",\n",
    "    \"type\": \"EXECUTION\",\n",
    "    \"executionType\": \"CLI\",\n",
    "    \"scheduler\" : \"FORK\",\n",
    "    \"environment\": null,\n",
    "    \"scratchDir\" : \"${SCRATCH_DIR}\",\n",
    "    \"queues\": [\n",
    "        {\n",
    "            \"name\": \"none\",\n",
    "            \"default\": true,\n",
    "            \"maxJobs\": 10,\n",
    "            \"maxUserJobs\": 10,\n",
    "            \"maxNodes\": 6,\n",
    "            \"maxProcessorsPerNode\": 6,\n",
    "            \"minProcessorsPerNode\": 1,\n",
    "            \"maxRequestedTime\": \"00:30:00\"\n",
    "        }\n",
    "    ],\n",
    "    \"login\": {\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        },\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SSH\"\n",
    "    },\n",
    "    \"maxSystemJobs\": 50,\n",
    "    \"maxSystemJobsPerUser\": 50,\n",
    "    \"storage\": {\n",
    "        \"host\": \"${AGAVE_SYSTEM_HOST}\",\n",
    "        \"port\": ${AGAVE_SYSTEM_PORT},\n",
    "        \"protocol\": \"SFTP\",\n",
    "        \"rootDir\": \"/\",\n",
    "        \"homeDir\": \"${AGAVE_STORAGE_HOME_DIR}\",\n",
    "        \"auth\": {\n",
    "          \"username\" : \"${MACHINE_USERNAME}\",\n",
    "          \"publicKey\" : \"${PUB_KEY}\",\n",
    "          \"privateKey\" : \"${PRIV_KEY}\",\n",
    "          \"type\" : \"SSHKEYS\"\n",
    "        }\n",
    "    },\n",
    "    \"workDir\": \"${AGAVE_STORAGE_WORK_DIR}\"\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added system sandbox-exec-dooley\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!systems-addupdate -F ${AGAVE_EXECUTION_SYSTEM_ID}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n",
      "/home/jovyan/.bash_logout\r\n",
      "/home/jovyan/.bashrc\r\n",
      "/home/jovyan/.cache\r\n",
      "/home/jovyan/.ngrok2\r\n"
     ]
    }
   ],
   "source": [
    "# Test to see if this worked...\n",
    "!files-list -l 5 -S ${AGAVE_EXECUTION_SYSTEM_ID} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the Application</h3>\n",
    "Agave allows us to describe custom allocations, limiting users to run a specific job. In this case, we're going to create a simple \"fork\" scheduler that just takes the command we want to run as a job parameter. The wrapper file is a shell script we will run on the execution machine. If we were using a scheduler, this would be our batch file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-wrapper.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-wrapper.txt\",\"\"\"\n",
    "#!/bin/bash\n",
    "\\${command}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Agave commands, we make a directory on the storage server an deploy our wrapper file there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully created folder agave-deploy\u001b[0m\n",
      "Uploading fork-wrapper.txt...\n",
      "######################################################################## 100.0%\n"
     ]
    }
   ],
   "source": [
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${AGAVE_APP_DEPLOYMENT_PATH}\n",
    "!files-upload -F fork-wrapper.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${AGAVE_APP_DEPLOYMENT_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All agave applications require a test file. The test file is a free form text file which allows you to specify what resources you might need to test your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-test.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-test.txt\",\"\"\"\n",
    "command=date\n",
    "fork-wrapper.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mSuccessfully created folder agave-deploy\u001b[0m\n",
      "Uploading fork-test.txt...\n",
      "######################################################################## 100.0%\n"
     ]
    }
   ],
   "source": [
    "!files-mkdir -S ${AGAVE_STORAGE_SYSTEM_ID} -N ${AGAVE_APP_DEPLOYMENT_PATH}\n",
    "!files-upload -F fork-test.txt -S ${AGAVE_STORAGE_SYSTEM_ID} ${AGAVE_APP_DEPLOYMENT_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like everything else in Agave, we describe our application with Json. We specifiy which machines the application will use, what method it will use for submitting jobs, job parameters and files, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `fork-app.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"fork-app.txt\",\"\"\"\n",
    "{  \n",
    "   \"name\":\"${AGAVE_USERNAME}-${MACHINE_NAME}-fork\",\n",
    "   \"version\":\"1.0\",\n",
    "   \"label\":\"Runs a command\",\n",
    "   \"shortDescription\":\"Runs a command\",\n",
    "   \"longDescription\":\"\",\n",
    "   \"deploymentSystem\":\"${AGAVE_STORAGE_SYSTEM_ID}\",\n",
    "   \"deploymentPath\":\"${AGAVE_APP_DEPLOYMENT_PATH}\",\n",
    "   \"templatePath\":\"fork-wrapper.txt\",\n",
    "   \"testPath\":\"fork-test.txt\",\n",
    "   \"executionSystem\":\"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"executionType\":\"CLI\",\n",
    "   \"parallelism\":\"SERIAL\",\n",
    "   \"modules\":[],\n",
    "   \"inputs\":[\n",
    "         {   \n",
    "         \"id\":\"datafile\",\n",
    "         \"details\":{  \n",
    "            \"label\":\"Data file\",\n",
    "            \"description\":\"\",\n",
    "            \"argument\":null,\n",
    "            \"showArgument\":false\n",
    "         },\n",
    "         \"value\":{  \n",
    "            \"default\":\"/dev/null\",\n",
    "            \"order\":0,\n",
    "            \"required\":false,\n",
    "            \"validator\":\"\",\n",
    "            \"visible\":true\n",
    "         }\n",
    "      }   \n",
    "   ],\n",
    "   \"parameters\":[{\n",
    "     \"id\" : \"command\",\n",
    "     \"value\" : {\n",
    "       \"visible\":true,\n",
    "       \"required\":true,\n",
    "       \"type\":\"string\",\n",
    "       \"order\":0,\n",
    "       \"enquote\":false,\n",
    "       \"default\":\"/bin/date\",\n",
    "       \"validator\":null\n",
    "     },\n",
    "     \"details\":{\n",
    "         \"label\": \"Command to run\",\n",
    "         \"description\": \"This is the actual command you want to run. ex. df -h -d 1\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     },\n",
    "     \"semantics\":{\n",
    "         \"label\": \"Command to run\",\n",
    "         \"description\": \"This is the actual command you want to run. ex. df -h -d 1\",\n",
    "         \"argument\": null,\n",
    "         \"showArgument\": false,\n",
    "         \"repeatArgument\": false\n",
    "     }\n",
    "   }],\n",
    "   \"outputs\":[]\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m\u001b[1;0mSuccessfully added app dooley-sandbox-fork-1.0\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!apps-addupdate -F fork-app.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running Jobs</h2>\n",
    "Now that we have specified our application using Agave, it is time to try running jobs. To start a job we, once again, create a Json file. The Json file describes the app, what resource to run on, as well as how and when to send notifications. Notifications are delivered by callback url. EMAIL is the easiest type to configure, but we show here how to send webhook notifications to the popular [RequestBin](https://requestb.in/). \n",
    "\n",
    "Before we configure our notification, we need to create a requestbin to use. There are convenience commands to interact with requestbin built into the Agave CLI. We will use those to get our URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rburl = !requestbin-create \n",
    "os.environ['REQUESTBIN_URL'] = rburl[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a URL to recieve webhooks from our job, Let's look at our job request. The way this job is configured, it will send the requestbin notifications for every job event until the job reaches a terminal state. For a full list of job events, please see http://docs.agaveplatform.org/#job-monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file `job.txt'\n"
     ]
    }
   ],
   "source": [
    "writefile(\"job.txt\",\"\"\"\n",
    " {\n",
    "   \"name\":\"fork-command-1\",\n",
    "   \"appId\": \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\",\n",
    "   \"executionSystem\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"archive\": false,\n",
    "   \"notifications\": [\n",
    "    {\n",
    "      \"url\":\"${REQUESTBIN_URL}?event=\\${EVENT}&jobid=\\${JOB_ID}\",\n",
    "      \"event\":\"*\",\n",
    "      \"persistent\":\"true\"\n",
    "    }\n",
    "   ],\n",
    "   \"parameters\": {\n",
    "     \"command\":\"echo hello\"\n",
    "   }\n",
    " }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the setvar() command can evalute `$()` style bash shell substitutions, we will use it to submit our job. This will capture the output of the submit command, and allow us to parse it for the JOB_ID. We'll use the JOB_ID in several subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT=Successfully submitted job 2320147314236920296-242ac114-0001-007\n",
      "JOB_ID=2320147314236920296-242ac114-0001-007\n"
     ]
    }
   ],
   "source": [
    "setvar(\"\"\"\n",
    "# Capture the output of the job submit command\n",
    "OUTPUT=$(jobs-submit -F job.txt)\n",
    "# Parse out the job id from the output\n",
    "JOB_ID=$(echo $OUTPUT | cut -d' ' -f4)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Job Monitoring and Output</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the job is running, the requestbin you registered will receive webhooks from Agave every time a job event occurs. To monitor this in real time, evaluate the next cell an visit the printed url in your browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://requestbin.agaveapi.co/1m41ypx1?inspect\n"
     ]
    }
   ],
   "source": [
    "print ('%s?inspect'%os.environ['REQUESTBIN_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can also monitor the job status by polling. Note that the notifications you receive via email and webhook are less wasteful of resources. However, we show you this for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT=PENDING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=FINISHED\n"
     ]
    }
   ],
   "source": [
    "for iter in range(20):\n",
    "    setvar(\"STAT=$(jobs-status $JOB_ID)\")\n",
    "    stat = os.environ[\"STAT\"]\n",
    "    sleep(5.0)\n",
    "    if stat == \"FINISHED\" or stat == \"FAILED\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs-history command provides you a record of the steps of what your job did. If your job fails for some reason, this is your best diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0mJob accepted and queued for submission.\r\n",
      "Skipping staging. No input data associated with this job.\r\n",
      "Preparing job for submission.\r\n",
      "Attempt 1 to submit job\r\n",
      "Fetching app assets from agave://sandbox-storage-dooley/agave-deploy\r\n",
      "Staging runtime assets to agave://sandbox-exec-dooley//home/jovyan/dooley/job-2320147314236920296-242ac114-0001-007-fork-command-1\r\n",
      "CLI job successfully forked as process id 435\r\n",
      "CLI job successfully forked as process id 435\r\n",
      "Job receieved duplicate RUNNING notification\r\n",
      "Job completed execution\r\n",
      "Job completed. Skipping archiving at user request.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-history ${JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command shows you the job id's and status of the last 5 jobs you ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m2320147314236920296-242ac114-0001-007 FINISHED\r\n",
      "6941711826161963496-242ac114-0001-007 FINISHED\r\n",
      "2139781772865302040-242ac114-0001-007 FINISHED\r\n",
      "6675412240218329576-242ac114-0001-007 FINISHED\r\n",
      "3260049706272615960-242ac114-0001-007 FINISHED\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-list -l 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next command provides you with a list of all the files generated by your job. You can use it to figure out which files you want to retrieve with jobs-output-get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;0m| type | length | name                  |\r\n",
      "| ---- | ------ | ----                  |\r\n",
      "| file | 68     | .agave.archive        |\r\n",
      "| file | 397    | .agave.log            |\r\n",
      "| file | 0      | fork-command-1.err    |\r\n",
      "| file | 2495   | fork-command-1.ipcexe |\r\n",
      "| file | 6      | fork-command-1.out    |\r\n",
      "| file | 4      | fork-command-1.pid    |\r\n",
      "| file | 29     | fork-test.txt         |\r\n",
      "| file | 22     | fork-wrapper.txt      |\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-list --rich --filter=type,length,name ${JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-get ${JOB_ID} fork-command-1.out\n",
    "!cat fork-command-1.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the standard error output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-output-get ${JOB_ID} fork-command-1.err\n",
    "!cat fork-command-1.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Automating</h3>\n",
    "Because we're working in Python, we can simply glue the above steps together and create a script to run jobs for us and fetch the standard output. Let's do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runagavecmd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runagavecmd.py\n",
    "from setvar import *\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def runagavecmd(cmd,infile=None):\n",
    "    setvar(\"REMOTE_COMMAND=\"+cmd)\n",
    "    setvar(\"REQUESTBIN_URL=$(requestbin-create)\")\n",
    "    print(\"\")\n",
    "    print(\" ** QUERY STRING FOR REQUESTBIN **\")\n",
    "    print('%s?inspect'%os.environ['REQUESTBIN_URL'])\n",
    "    print(\"\")\n",
    "    # The input file is an optional parameter, both\n",
    "    # to our function and to the Agave application.\n",
    "    if infile == None:\n",
    "        setvar(\"INPUTS={}\")\n",
    "    else:\n",
    "        setvar('INPUTS={\"datafile\":\"'+infile+'\"}')\n",
    "    setvar(\"JOB_FILE=job-remote-$PID.txt\")\n",
    "    # Create the Json for the job file.\n",
    "    writefile(\"$JOB_FILE\",\"\"\"\n",
    " {\n",
    "   \"name\":\"fork-command-1\",\n",
    "   \"appId\": \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\",\n",
    "   \"executionSystem\": \"${AGAVE_EXECUTION_SYSTEM_ID}\",\n",
    "   \"archive\": false,\n",
    "   \"notifications\": [\n",
    "    {\n",
    "      \"url\":\"${REQUESTBIN_URL}?event=\\${EVENT}&jobid=\\${JOB_ID}\",\n",
    "      \"event\":\"*\",\n",
    "      \"persistent\":\"true\"\n",
    "    }\n",
    "   ],\n",
    "   \"parameters\": {\n",
    "     \"command\":\"${REMOTE_COMMAND}\"\n",
    "   },\n",
    "   \"inputs\":${INPUTS}\n",
    " }\"\"\")\n",
    "    # Run the job and capture the output.\n",
    "    setvar(\"\"\"\n",
    "# Capture the output of the job submit command\n",
    "OUTPUT=$(jobs-submit -F $JOB_FILE)\n",
    "# Parse out the job id from the output\n",
    "JOB_ID=$(echo $OUTPUT | cut -d' ' -f4)\n",
    "\"\"\")\n",
    "    # Poll and wait for the job to finish.\n",
    "    for iter in range(80): # Excessively generous\n",
    "        setvar(\"STAT=$(jobs-status $JOB_ID)\")\n",
    "        stat = os.environ[\"STAT\"]\n",
    "        sleep(5.0)\n",
    "        if stat == \"FINISHED\" or stat == \"FAILED\":\n",
    "            break\n",
    "    # Fetch the job output from the remote machine\n",
    "    setvar(\"CMD=jobs-output-get ${JOB_ID} fork-command-1.out\")\n",
    "    os.system(os.environ[\"CMD\"])\n",
    "    print(\"All done! Output follows.\")\n",
    "    # Load the output into memory\n",
    "    output=readfile(\"fork-command-1.out\")\n",
    "    print(\"=\" * 70)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'runagavecmd' from '/home/jovyan/agave/runagavecmd.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import runagavecmd as r\n",
    "import imp\n",
    "imp.reload(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOTE_COMMAND=lscpu\n",
      "REQUESTBIN_URL=https://requestbin.agaveapi.co/1nl9tv81\n",
      "\n",
      " ** QUERY STRING FOR REQUESTBIN **\n",
      "https://requestbin.agaveapi.co/1nl9tv81?inspect\n",
      "\n",
      "INPUTS={}\n",
      "JOB_FILE=job-remote-2130.txt\n",
      "Writing file `job-remote-2130.txt'\n",
      "OUTPUT=Successfully submitted job 6504819850075640296-242ac114-0001-007\n",
      "JOB_ID=6504819850075640296-242ac114-0001-007\n",
      "STAT=PENDING\n",
      "STAT=STAGED\n",
      "STAT=SUBMITTING\n",
      "STAT=STAGED\n",
      "STAT=STAGED\n",
      "STAT=PROCESSING_INPUTS\n",
      "STAT=STAGING_INPUTS\n",
      "STAT=STAGED\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=SUBMITTING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=RUNNING\n",
      "STAT=FINISHED\n",
      "CMD=jobs-output-get 7597588379196920296-242ac114-0001-007 fork-command-1.out\n",
      "All done! Output follows.\n",
      "Reading file `fork-command-1.out'\n",
      "======================================================================\n",
      "Sending build context to Docker daemon  13.31kB\n",
      "\n",
      "Step 1/19 : FROM stevenrbrandt/science-base\n",
      " ---> e93db47f971c\n",
      "Step 2/19 : LABEL baseImage \"stevenrbrandt/science-base:latest\"\n",
      " ---> Using cache\n",
      " ---> 1fe963ea7582\n",
      "Step 3/19 : LABEL version \"3\"\n",
      " ---> Using cache\n",
      " ---> f8e945f44a9c\n",
      "Step 4/19 : LABEL software \"FUNWAVE-TVD\"\n",
      " ---> Using cache\n",
      " ---> 8b997217e1ed\n",
      "Step 5/19 : LABEL softwareVersion \"v3.2-beta\"\n",
      " ---> Using cache\n",
      " ---> 2f274303db68\n",
      "Step 6/19 : LABEL description \"FUNWAVE–TVD is the TVD version of the fully nonlinear Boussinesq wave model (FUNWAVE) initially developed by Kirby et al. (1998)\"\n",
      " ---> Using cache\n",
      " ---> 33d2ef1660e5\n",
      "Step 7/19 : LABEL website \"https://fengyanshi.github.io/build/html/index.html\"\n",
      " ---> Using cache\n",
      " ---> 3f84e5ec0aaa\n",
      "Step 8/19 : LABEL documentation \"https://fengyanshi.github.io/build/html/index.html\"\n",
      " ---> Using cache\n",
      " ---> 4585eb02a0b3\n",
      "Step 9/19 : LABEL license \"BSD 3-Clause\"\n",
      " ---> Running in 45e73c41b397\n",
      " ---> cd34db9a1efb\n",
      "Removing intermediate container 45e73c41b397\n",
      "Step 10/19 : LABEL tags \"crc,fortran,tvd\"\n",
      " ---> Running in 39faa5ef7f7b\n",
      " ---> 2ebcbf528582\n",
      "Removing intermediate container 39faa5ef7f7b\n",
      "Step 11/19 : MAINTAINER Steven R. Brandt <sbrandt@cct.lsu.edu>\n",
      " ---> Running in fa8f6274dfa6\n",
      " ---> 16e682d83250\n",
      "Removing intermediate container fa8f6274dfa6\n",
      "Step 12/19 : USER root\n",
      " ---> Running in 714214cbc604\n",
      " ---> ac1e30e69e4a\n",
      "Removing intermediate container 714214cbc604\n",
      "Step 13/19 : RUN mkdir -p /home/install\n",
      " ---> Running in 6ed65aede494\n",
      " ---> 6b1c4cb40af9\n",
      "Removing intermediate container 6ed65aede494\n",
      "Step 14/19 : RUN chown jovyan /home/install\n",
      " ---> Running in 048a460eeed6\n",
      " ---> b6c10b7b2ec9\n",
      "Removing intermediate container 048a460eeed6\n",
      "Step 15/19 : USER jovyan\n",
      " ---> Running in 500a6303edc0\n",
      " ---> af6910240894\n",
      "Removing intermediate container 500a6303edc0\n",
      "Step 16/19 : RUN cd /home/install &&     git clone https://github.com/fengyanshi/FUNWAVE-TVD &&     cd FUNWAVE-TVD/src &&     perl -p -i -e 's/FLAG_8 = -DCOUPLING/#$&/' Makefile &&     make\n",
      " ---> Running in 1674c01e685c\n",
      "\u001b[91mCloning into 'FUNWAVE-TVD'...\n",
      "\u001b[0m/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_param.F > mod_param.f90\n",
      "mpif90  -c     mod_param.f90\n",
      "/bin/rm mod_param.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_global.F > mod_global.f90\n",
      "mpif90  -c     mod_global.f90\n",
      "/bin/rm mod_global.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_input.F > mod_input.f90\n",
      "mpif90  -c     mod_input.f90\n",
      "/bin/rm mod_input.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_vessel.F > mod_vessel.f90\n",
      "mpif90  -c     mod_vessel.f90\n",
      "/bin/rm mod_vessel.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_bathy_correction.F > mod_bathy_correction.f90\n",
      "mpif90  -c     mod_bathy_correction.f90\n",
      "/bin/rm mod_bathy_correction.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_meteo.F > mod_meteo.f90\n",
      "mpif90  -c     mod_meteo.f90\n",
      "/bin/rm mod_meteo.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mod_parallel_field_io.F > mod_parallel_field_io.f90\n",
      "mpif90  -c     mod_parallel_field_io.f90\n",
      "/bin/rm mod_parallel_field_io.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     main.F > main.f90\n",
      "mpif90  -c     main.f90\n",
      "/bin/rm main.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     bc.F > bc.f90\n",
      "mpif90  -c     bc.f90\n",
      "/bin/rm bc.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     fluxes.F > fluxes.f90\n",
      "mpif90  -c     fluxes.f90\n",
      "/bin/rm fluxes.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     init.F > init.f90\n",
      "mpif90  -c     init.f90\n",
      "/bin/rm init.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     io.F > io.f90\n",
      "mpif90  -c     io.f90\n",
      "/bin/rm io.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     tridiagnal.F > tridiagnal.f90\n",
      "mpif90  -c     tridiagnal.f90\n",
      "/bin/rm tridiagnal.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     breaker.F > breaker.f90\n",
      "mpif90  -c     breaker.f90\n",
      "/bin/rm breaker.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     derivatives.F > derivatives.f90\n",
      "mpif90  -c     derivatives.f90\n",
      "/bin/rm derivatives.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     dispersion.F > dispersion.f90\n",
      "mpif90  -c     dispersion.f90\n",
      "/bin/rm dispersion.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     etauv_solver.F > etauv_solver.f90\n",
      "mpif90  -c     etauv_solver.f90\n",
      "/bin/rm etauv_solver.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     sponge.F > sponge.f90\n",
      "mpif90  -c     sponge.f90\n",
      "/bin/rm sponge.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     sources.F > sources.f90\n",
      "mpif90  -c     sources.f90\n",
      "/bin/rm sources.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     masks.F > masks.f90\n",
      "mpif90  -c     masks.f90\n",
      "/bin/rm masks.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     parallel.F > parallel.f90\n",
      "mpif90  -c     parallel.f90\n",
      "/bin/rm parallel.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     statistics.F > statistics.f90\n",
      "mpif90  -c     statistics.f90\n",
      "/bin/rm statistics.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     wavemaker.F > wavemaker.f90\n",
      "mpif90  -c     wavemaker.f90\n",
      "/bin/rm wavemaker.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     mixing.F > mixing.f90\n",
      "mpif90  -c     mixing.f90\n",
      "/bin/rm mixing.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     nesting.F > nesting.f90\n",
      "mpif90  -c     nesting.f90\n",
      "/bin/rm nesting.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     misc.F > misc.f90\n",
      "mpif90  -c     misc.f90\n",
      "/bin/rm misc.f90\n",
      "/usr/bin/cpp  -P -traditional  -P -traditional  -DDOUBLE_PRECISION  -DPARALLEL  -DCARTESIAN                     samples.F > samples.f90\n",
      "mpif90  -c     samples.f90\n",
      "/bin/rm samples.f90\n",
      "mpif90     -o funwave_vessel mod_param.o mod_global.o mod_input.o mod_vessel.o mod_bathy_correction.o mod_meteo.o mod_parallel_field_io.o main.o bc.o fluxes.o init.o io.o tridiagnal.o breaker.o derivatives.o dispersion.o etauv_solver.o sponge.o sources.o masks.o parallel.o statistics.o wavemaker.o mixing.o nesting.o misc.o samples.o \n",
      " ---> 55bbac42b3ca\n",
      "Removing intermediate container 1674c01e685c\n",
      "Step 17/19 : WORKDIR /home/install/FUNWAVE-TVD/src\n",
      " ---> cd188a3cc8cc\n",
      "Removing intermediate container 5511d6e350eb\n",
      "Step 18/19 : RUN mkdir -p /home/jovyan/rundir\n",
      " ---> Running in dfbf40c34bdc\n",
      " ---> f42f46ab47e2\n",
      "Removing intermediate container dfbf40c34bdc\n",
      "Step 19/19 : WORKDIR /home/jovyan/rundir\n",
      " ---> 34d25d6a8ce1\n",
      "Removing intermediate container 2f8ed63f5778\n",
      "Successfully built 34d25d6a8ce1\n",
      "Successfully tagged funwave-tvd-2:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r.runagavecmd(\"lscpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Permissions and Sharing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the users and the permssions they have to look at the given job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!jobs-pems-list ${JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now pair off with your neighbor and both of you share your job with them.\n",
    "# For now, just give read access\n",
    "!jobs-pems-update -u training002 -p READ ${JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's see if we can see our neighbor's job\n",
    "# Now let's see if we can see our neighbor's job\n",
    "shared_job = !jobs-search --filter=id -l 1 owner.neq=${AGAVE_USERNAME} \n",
    "os.environ['SHARED_JOB_ID'] = shared_job[0]\n",
    "print(os.environ['SHARED_JOB_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permissions are just that, permitting someone to do something. You said your neighbor could view your job. Let's see what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You already searched for the job and found it, so you should be able to lis\n",
    "# an view the details\n",
    "!jobs-list $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You should also be able to view the history. Here we'll just return the last few \n",
    "# events. Notice the history event showed up history event\n",
    "!jobs-history --limit 3 --order desc $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also view their job output\n",
    "!jobs-output-list -L $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What if we no longer want to see the job. Let's delete it.\n",
    "!jobs-delete $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doah! We can't delete the shared job because we weren't granted write permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's grant write access and see what we can do\n",
    "!jobs-pems-update -u training002 -p READ_WRITE ${JOB_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's see if we can delete the shared job\n",
    "!jobs-delete $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wait, now we don't have anything to work with. \n",
    "# No worries. Agave doens't really delete anything. Your job is still there\n",
    "# We just need to restore it.\n",
    "!jobs-restore $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's try to rerun the job\n",
    "!jobs-resubmit $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Well, what app did they use in the job?\n",
    "\n",
    "shared_job = !jobs-list -v --filter=executionSystem,appId $SHARED_JOB_ID | jq -r '. | [ .executionSystem , .appId] | .[]'\n",
    "print(shared_job)\n",
    "os.environ['SHARED_JOB_APP'] = shared_job[1]\n",
    "os.environ['SHARED_JOB_SYSTEM'] = shared_job[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hmm, do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Oh, we don't have permission to even view the app. Guess our job permissions\n",
    "# don't extend to the application. Let's be a good neighbor and share our apps\n",
    "# with each other\n",
    "! apps-pems-update -u training002 -p READ \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score, but wait, do I need execute to run? We should granb that too.\n",
    "# Hmm, do we have access to the app?\n",
    "! apps-pems-update -u training002 -p EXECUTE \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I guess permissions aren't hierachical. Now i can execute it (I think), but I can't\n",
    "# read it. How aabout we grant read_execute instead\n",
    "! apps-pems-update -u training002 -p READ_EXECUTE \"${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now do we have access to the app?\n",
    "! apps-pems-list $SHARED_JOB_APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So now we can rerun our neighbor's job, right\n",
    "!jobs-resubmit -v $SHARED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drat. why can't we run now? Do we have system access?\n",
    "!systems-roles-list $SHARED_JOB_SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ok, let's skip to the end. we'll just realize we should grant a user rather than guest role\n",
    "# to the system\n",
    "!systems-roles-addupdate -u training002 -r USER $AGAVE_EXECUTION_SYSTEM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# that should work, right?\n",
    "!systems-roles-list $SHARED_JOB_SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So can we run the job now?\n",
    "resubmitted_job_id = ! jobs-resubmit -v --filter=id $SHARED_JOB_ID | jq -r '.id'\n",
    "os.environ['RESUBMITTED_JOB_ID'] = resubmitted_job_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yay. wait, who owns the data?\n",
    "print (resubmitted_job_id[0])\n",
    "! jobs-pems-list $RESUBMITTED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine, mine\n",
    "# kill it, we're moving on.\n",
    "! jobs-stop $RESUBMITTED_JOB_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also share data a few ways\n",
    "job_output_url = ! jobs-output-list -v --filter=_links $JOB_ID fork-command-1.out | jq -r '.[0]._links.self.href'\n",
    "os.environ['JOB_OUTPUT_URL'] = job_output_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postit_url = ! postits-create -m 3 -l 86400 -V $JOB_OUTPUT_URL | jq -r '.result._links.self.href' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# click on the link a few times to see it working.\n",
    "print (postit_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can also share your data via the files api\n",
    "# let's share the job directory with each other\n",
    "job_path = ! jobs-list -v $JOB_ID | jq -r '.outputPath'\n",
    "os.environ['JOB_OUTPUT_FOLDER_PATH'] = job_path[0]\n",
    "! files-pems-update -u training002 -p read -S $AGAVE_EXECUTION_SYSTEM_ID $JOB_OUTPUT_FOLDER_PATH/fork-command-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mPlease run /home/jovyan/src/cli/bin/tenants-init to initialize your client before attempting to interact with the APIs.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jobs-delete $JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Data\n",
    "\n",
    "You can also use Agave to manage your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Agave ToGo web portal  \n",
    "\n",
    "Follow the link below to run your job from a web portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo http://togo.agaveplatform.org/app/#/apps/${AGAVE_USERNAME}-${MACHINE_NAME}-fork-1.0/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
